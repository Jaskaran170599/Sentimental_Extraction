{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentencepiece_pb2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-adc0afcf34b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msentencepiece\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msentencepiece_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentencepiece_pb2'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import sentencepiece_pb2 as spt\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "\"MAX_LEN\" : 96,\n",
    "\"TOKENIZER\" : tokenizers.ByteLevelBPETokenizer(\"./model/vocab.json\",\n",
    "                                               \"./model/merges.txt\",\n",
    "                                               lowercase=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(length=128):\n",
    "    ids=tf.keras.layers.Input(shape=(length,),name=\"input_ids\",dtype=tf.int32)\n",
    "    token_ids=tf.keras.layers.Input(shape=(length,),name=\"token_type_ids\",dtype=tf.int32)    \n",
    "    att_mask=tf.keras.layers.Input(shape=(length,),name=\"attention_mask\",dtype=tf.int32)\n",
    "    \n",
    "    config = RobertaConfig.from_pretrained(\"./model/\", output_hidden_states=True)\n",
    "    BL=TFRobertaModel(config=config)\n",
    "    \n",
    "    _,_,layers=BL(ids,attention_mask=att_mask,token_type_ids=token_ids,training=False)\n",
    "    drop=tf.keras.layers.Dropout(0.1)(layers[-1])\n",
    "    start=tf.keras.layers.Dense(2,activation=\"sigmoid\")(drop)\n",
    "    model=tf.keras.models.Model(inputs=[ids,token_ids,att_mask],outputs=[start])\n",
    "    \n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.binary_crossentropy,optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1bfd55d4860>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=get_model(config[\"MAX_LEN\"])\n",
    "model.load_weights(\"./model/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(text,sentiment):\n",
    "    text = \" \"+\" \".join(str(text).split())\n",
    "    \n",
    "    encoded_text=config[\"TOKENIZER\"].encode(text)\n",
    "    ids=encoded_text.ids\n",
    "\n",
    "    sentiment_id = {\n",
    "    'positive': 1313,\n",
    "    'negative': 2430,\n",
    "    'neutral': 7974\n",
    "    }\n",
    "\n",
    "    ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + ids + [2]\n",
    "    type_ids = [0, 0, 0, 0] + [0] * (len(ids)-4)\n",
    "    attention = [1] * len(type_ids)\n",
    "        \n",
    "    if(len(ids)>config[\"MAX_LEN\"]):\n",
    "        ids=ids[:config[\"MAX_LEN\"]-5]\n",
    "        type_ids=type_ids[:config[\"MAX_LEN\"]-5]\n",
    "        attention=attention[:config[\"MAX_LEN\"]-5]\n",
    "        \n",
    "    pad=config[\"MAX_LEN\"]-len(ids)\n",
    "\n",
    "    ids=np.array(ids+[0]*pad).reshape((1,-1))\n",
    "    type_ids=np.array(type_ids+[0]*pad).reshape((1,-1))\n",
    "    attention=np.array(attention+[0]*pad).reshape((1,-1))\n",
    "\n",
    "    return {\"orig\":text,\"input_ids\":ids,\"token_type_ids\":type_ids,\"attention_mask\":attention,\"sentiments\":sentiment}\n",
    "    \n",
    "def get_text(text,pred,sentiments):\n",
    "        pred=np.argmax(pred,axis=1)[0]\n",
    "        \n",
    "        if(len(text.split())<2):\n",
    "            return text\n",
    "            \n",
    "        t=config[\"TOKENIZER\"].encode(text).offsets\n",
    "        t=[(0, 0)] * 4 + t + [(0, 0)]\n",
    "        i,j=pred[0],pred[1]\n",
    "        return text[t[i][0]:t[j][1]+1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(text,sentiment):\n",
    "    preprocess=get_target(text,sentiment)\n",
    "    pred=model.predict(preprocess)\n",
    "    return get_text(text,pred,sentiment).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'dict'> containing {\"<class 'str'>\"} keys and {\"<class 'numpy.ndarray'>\", \"<class 'str'>\"} values), <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'really good'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='Shanghai is also really good '\n",
    "sentiment=\"positive\"\n",
    "get_result(text,sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
